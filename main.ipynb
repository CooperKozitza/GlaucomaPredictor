{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LWvRJ-g8ZuSK"
      },
      "source": [
        "# Import Data\n",
        "\n",
        "Create a pandas Dataframe by reading `data.json`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 652
        },
        "id": "kSYK1qSRaUGx",
        "outputId": "a80d89d9-2c75-4ab5-fc3a-95ab5191e2c0"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_json('data.json')\n",
        "\n",
        "df['Average BMI'] = df[['Minimum BMI', 'Max BMI']].mean(axis=1)\n",
        "df['Average BP'] = df[['Diastolic BP', 'Systolic BP']].mean(axis=1)\n",
        "\n",
        "df.drop(columns=['Minimum BMI', 'Max BMI', 'Diastolic BP', 'Systolic BP'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxyXwcTdZ5z0"
      },
      "source": [
        "# Data Preprocessing\n",
        "First we catagorize the columns based on the type of information they contain so they can later be encoded. Columns that contain numerical information, like age or IOP, are put into `numerical_cols`, while columns that contain information that can be found in a set of catagories, like *Gender*, which can be either `'MALE'` or `'FEMALE'`, is put in `categorical_cols`\n",
        "\n",
        "## Removing `NaN` and Missing Values\n",
        "For each of the numerical columns, we replace missing values with `-1`, and for the categorical columns we replace missing values with `'NONE'`\n",
        "\n",
        "## Scaling and Encoding Values\n",
        "The ColumnTransformer is set up with three transformers:\n",
        "- `'num'` applies `StandardScaler` to the numerical columns, which is used to standardize (`mean=0`, `standard deviation=1`) numerical features.\n",
        "- `'cat'` applies `OneHotEncoder` to the categorical columns, which converts categorical variables into a form that could be provided to ML algorithms (dummy variables).\n",
        "- `'passthrough'` simply passes the specified columns through without any changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 478
        },
        "id": "BG5PpRArCDRb",
        "outputId": "47a94c74-83f9-4f2c-8b86-41e6bdadca06"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "numerical_cols = ['Age', 'Average BMI', 'SPO2', 'Average BP', 'Total Med Count']\n",
        "categorical_cols = ['Race', 'Tobacco Use', 'Alcohol Use']\n",
        "binary_categorical_cols = ['Gender']\n",
        "passthrough_cols = ['Glaucoma']\n",
        "\n",
        "numerical_imputer = SimpleImputer(strategy='mean')\n",
        "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', Pipeline(steps=[('imputer', numerical_imputer),\n",
        "                                ('scaler', StandardScaler())]), numerical_cols),\n",
        "        ('cat', Pipeline(steps=[('imputer', categorical_imputer),\n",
        "                                ('encoder', OneHotEncoder())]), categorical_cols),\n",
        "        ('lab', OrdinalEncoder(), binary_categorical_cols),\n",
        "        ('passthrough', 'passthrough', passthrough_cols)\n",
        "    ])\n",
        "\n",
        "# Display an example of the preprocessed data by creating a DataFrame, then displaying it\n",
        "transformed_data = preprocessor.fit_transform(df)\n",
        "new_column_names = numerical_cols + \\\n",
        "                   list(preprocessor.named_transformers_['cat'].get_feature_names_out(categorical_cols)) + \\\n",
        "                   binary_categorical_cols + \\\n",
        "                   passthrough_cols\n",
        "\n",
        "transformed_df = pd.DataFrame(transformed_data, columns=new_column_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Slit Data Into Training and Testing Sets\n",
        "First, split the glaucoma column apart from the data, then split the data into 80% training data and 20% testing data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aF35xfH9Eydn"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = transformed_df.drop('Glaucoma', axis=1)\n",
        "y = transformed_df['Glaucoma']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Create Models\n",
        "Create each base model that will become a part of the stack and use hyperparameter-tuning to optimize each model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "'''\n",
        "rf_param_grid = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'max_features': ['auto', 'sqrt', 'log2'],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'bootstrap': [True, False]\n",
        "}\n",
        "\n",
        "rf_grid_search = GridSearchCV(RandomForestClassifier(random_state=42), rf_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "rf_grid_search.fit(X_train, y_train)\n",
        "best_rf_model = rf_grid_search.best_estimator_\n",
        "\n",
        "print('RF params:', rf_grid_search.best_params_)\n",
        "'''\n",
        "\n",
        "best_rf_model = RandomForestClassifier(\n",
        "    random_state=42, \n",
        "    n_estimators=200, \n",
        "    max_features='sqrt', \n",
        "    min_samples_split=10, \n",
        "    max_depth=10, \n",
        "    min_samples_leaf=2, \n",
        "    bootstrap=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "\n",
        "'''\n",
        "gb_param_grid = {\n",
        "    'n_estimators': [200],  # Number of boosting stages to be run\n",
        "    'learning_rate': [0.03],  # Shrinks the contribution of each tree by learning_rate\n",
        "    'max_depth': [4],  # Maximum depth of the individual regression estimators\n",
        "    'min_samples_split': [6],  # Minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': [2],  # Minimum number of samples required to be at a leaf node\n",
        "    'max_features': [None],  # The number of features to consider when looking for the best split\n",
        "    'subsample': [0.5, 1.0]  # The fraction of samples to be used for fitting the individual base learners\n",
        "}\n",
        "\n",
        "gb_grid_search = GridSearchCV(GradientBoostingClassifier(random_state=42), gb_param_grid, cv=5, n_jobs=-1, verbose=2)\n",
        "gb_grid_search.fit(X_train, y_train)\n",
        "best_gb_model = gb_grid_search.best_estimator_\n",
        "\n",
        "print('GB params:', gb_grid_search.best_params_)\n",
        "'''\n",
        "\n",
        "best_gb_model = GradientBoostingClassifier(\n",
        "    random_state=42, \n",
        "    n_estimators=200, \n",
        "    learning_rate=0.03, \n",
        "    max_depth=4, \n",
        "    min_samples_split=6, \n",
        "    min_samples_leaf=2, \n",
        "    max_features=None, \n",
        "    subsample=1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## C Support Vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "'''\n",
        "svc_param_grid = {\n",
        "    'C': [0.1, 1],  # Regularization parameter\n",
        "    'gamma': [1, 0.1],  # Kernel coefficient for 'rbf', 'poly' and 'sigmoid'\n",
        "    'kernel': ['rbf', 'poly', 'sigmoid'],  # Specifies the kernel type to be used in the algorithm\n",
        "    'degree': [2, 3],  # Degree of the polynomial kernel function ('poly'). Ignored by all other kernels.\n",
        "    'class_weight': [None, 'balanced'],  # Set the parameter C of class i to class_weight[i]*C for SVC. \n",
        "}\n",
        "\n",
        "\n",
        "svc_grid_search = GridSearchCV(SVC(), svc_param_grid, cv=3, n_jobs=-1, verbose=2)\n",
        "svc_grid_search.fit(X_train, y_train)\n",
        "best_svc_model = svc_grid_search.best_estimator_\n",
        "\n",
        "print('SVC params:', svc_grid_search.best_params_)\n",
        "'''\n",
        "\n",
        "best_svc_model = SVC(\n",
        "    C=1, \n",
        "    class_weight=None, \n",
        "    degree=2, \n",
        "    gamma=0.1, \n",
        "    kernel='poly')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom Neural Network\n",
        "The data is best predicted by a neural network because the patterns in the data are well-captured by the architectures and feature hierarchies that deep learning models, like those built with Keras, can learn. Deep learning models, particularly those with multiple layers, are very effective at automatically detecting complex patterns and interactions between features without the need for manual feature engineering."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Defining and Compiling the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def create_neural_network(input_dim):\n",
        "    model = models.Sequential([\n",
        "        tf.keras.Input(shape=(input_dim,)),  # Input layer specifying the shape\n",
        "        layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "        layers.Dropout(0.2),\n",
        "        layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # Compile the model\n",
        "    model.compile(optimizer='adam',\n",
        "                loss='binary_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Wrapping the Model in `KerasClassifier`\n",
        "Also implement early stopping to prevent over-fitting"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "class KerasClassifierWrapper(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, input_dim, epochs=100, batch_size=10, validation_split=0.2):\n",
        "        self.input_dim = input_dim\n",
        "        self.epochs = epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.validation_split = validation_split\n",
        "\n",
        "    def fit(self, X, y, **kwargs):\n",
        "        self.model = self._build_model()\n",
        "        early_stopping = EarlyStopping(\n",
        "            monitor='val_loss',\n",
        "            patience=10,\n",
        "            verbose=1,\n",
        "            restore_best_weights=True\n",
        "        )\n",
        "        self.model.fit(X, y, epochs=self.epochs, batch_size=self.batch_size,\n",
        "                       validation_split=self.validation_split, callbacks=[early_stopping], verbose=0, **kwargs)\n",
        "        return self\n",
        "\n",
        "    def _build_model(self):\n",
        "        model = models.Sequential([\n",
        "            tf.keras.Input(shape=(self.input_dim,)),  # Input layer specifying the shape\n",
        "            layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "            layers.Dropout(0.2),\n",
        "            layers.Dense(1, activation='sigmoid')\n",
        "        ])\n",
        "        model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "        return model\n",
        "\n",
        "    def predict(self, X, **kwargs):\n",
        "        predictions = self.model.predict(X, **kwargs)\n",
        "        return (predictions > 0.5).astype(int)\n",
        "\n",
        "    def score(self, X, y, **kwargs):\n",
        "        loss, accuracy = self.model.evaluate(X, y, **kwargs)\n",
        "        return accuracy\n",
        "\n",
        "best_nn_model = KerasClassifierWrapper(input_dim=X_train.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Define and Tune Stacking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "\n",
        "base_models = [\n",
        "    ('random_forest', best_rf_model),\n",
        "    ('gradient_boosting', best_gb_model),\n",
        "    ('svc', best_svc_model),\n",
        "    ('neural_net', best_nn_model)\n",
        "]\n",
        "\n",
        "meta_param_grid = {\n",
        "    'C': [0.01, 0.1, 1, 10, 100],  # Regularization strength\n",
        "    'solver': ['lbfgs', 'liblinear'],  # Algorithm to use in the optimization problem\n",
        "    'penalty': ['l2'],  # Regularization penalty (depending on the solver, you might explore 'l1' as well)\n",
        "}\n",
        "\n",
        "meta_model_grid_search = GridSearchCV(LogisticRegression(max_iter=10000), meta_param_grid, cv=5, n_jobs=-1, verbose=3)\n",
        "\n",
        "stacking_model = StackingClassifier(estimators=base_models, final_estimator=LogisticRegression(max_iter=10000), cv=5)\n",
        "\n",
        "# Fit the stacking model\n",
        "stacking_model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4estU6lWYCXM"
      },
      "source": [
        "# Determine Accuracy\n",
        "Accuracy is determined by letting the model predict the target of the data previously reserved for testing, and then comparing it to the actual values for that data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9E7cfKRaNCXN",
        "outputId": "6b214bec-9f86-4197-b348-13cbde363658"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.utils.validation import check_is_fitted\n",
        "\n",
        "# Accuracy of individual models\n",
        "for name, model in base_models:\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "    print(f'{name} Model Accuracy: {accuracy}')\n",
        "\n",
        "y_pred = stacking_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Stacking Model Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NgMoLhkVYEmN"
      },
      "source": [
        "# Plot Importance of Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 811
        },
        "id": "2BtAC-CVMTea",
        "outputId": "9632e7e8-b9ee-4b48-e3d7-34b568d3e3f8"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "for name, model in stacking_model.estimators:\n",
        "    print(f'{name} {hasattr(model, 'feature_importances_')}')\n",
        "    if hasattr(model, 'feature_importances_'):\n",
        "        # Plot feature importances\n",
        "        plt.figure(figsize=(12, 8))\n",
        "        plt.title(f\"Feature Importances for {name}\")\n",
        "        importances = model.feature_importances_\n",
        "        indices = np.argsort(importances)\n",
        "        plt.barh(range(len(importances)), importances[indices], color='skyblue')\n",
        "        plt.yticks(range(len(importances)), np.array(X_train.columns)[indices])\n",
        "        plt.xlabel(\"Importance\")\n",
        "        plt.ylabel(\"Feature\")\n",
        "        plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.12.0 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "vscode": {
      "interpreter": {
        "hash": "e03048ec38c9efb205561778c9d01c5db91c68b2ba0b560ef345aff3262df493"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
